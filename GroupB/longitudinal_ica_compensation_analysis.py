#!/usr/bin/env python3
"""
Longitudinal ICA Compensation Analysis for SCA
==============================================

This script implements your research goal: mapping how individual SCA patients' brains 
compensate for damage over time using ICA to identify early biomarkers.

Research Objective:
- Map individual compensation strategies across rs-1, rs-2, rs-3
- Identify early compensation patterns in rs-1 that predict rs-2/rs-3 outcomes
- Create personalized "compensation maps" for each patient
- Prioritize earliest possible biomarkers for clinical intervention

Author: Your Research Team
Date: 2024
"""

import os
import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import time
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from sklearn.decomposition import FastICA, PCA
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from scipy import stats
from scipy.stats import mannwhitneyu, ttest_ind, pearsonr

from nilearn.input_data import NiftiMasker
from nilearn import datasets, plotting
from nilearn.plotting import plot_glass_brain, plot_stat_map

class LongitudinalICACompensationAnalyzer:
    """
    Analyze brain compensation patterns across timepoints using ICA for early biomarker discovery
    """
    
    def __init__(self, data_dir="extracted data/raw_data", output_dir="longitudinal_ica_compensation_results"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Data containers
        self.data_by_timepoint = {}  # Organized by timepoint
        self.data_by_subject = {}    # Organized by subject for longitudinal tracking
        self.compensation_maps = {}  # Individual compensation patterns
        self.early_biomarkers = {}  # rs-1 patterns that predict rs-2/rs-3
        
        # Analysis parameters
        self.n_components = 20  # Number of ICA components
        self.timepoints = ['rs-1', 'rs-2', 'rs-3']
        
        print("ðŸ§  LONGITUDINAL ICA COMPENSATION ANALYSIS")
        print("=" * 60)
        print("Goal: Map individual SCA brain compensation strategies over time")
        print("Method: ICA + Longitudinal prediction + Early biomarker discovery")
        print("=" * 60)
        
    def load_longitudinal_data(self):
        """Load and organize data by both timepoint and subject for longitudinal analysis"""
        print("\nðŸ“‚ Loading longitudinal brain imaging data...")
        
        # Get all NII files
        all_files = list(self.data_dir.glob("*.nii.gz"))
        print(f"Found {len(all_files)} total brain scan files")
        
        # Initialize data structures
        for timepoint in self.timepoints:
            self.data_by_timepoint[timepoint] = {'patients': [], 'controls': []}
        
        # Process each file
        for file_path in tqdm(all_files, desc="Organizing files"):
            filename = file_path.name
            
            # Extract information from filename
            if 'Denoised_' in filename:
                parts = filename.replace('Denoised_', '').split('_')
                if len(parts) >= 3:
                    subject_id = parts[0]  # p01, p02, C01, C02, etc.
                    timepoint = parts[1]   # rs-1, rs-2, rs-3
                    
                    # Determine group
                    group = 'patients' if subject_id.startswith('p') else 'controls'
                    
                    # Store file info
                    file_info = {
                        'file_path': str(file_path),
                        'subject_id': subject_id,
                        'timepoint': timepoint,
                        'group': group,
                        'filename': filename
                    }
                    
                    # Add to timepoint organization
                    if timepoint in self.timepoints:
                        self.data_by_timepoint[timepoint][group].append(file_info)
                    
                    # Add to subject organization
                    if subject_id not in self.data_by_subject:
                        self.data_by_subject[subject_id] = {
                            'group': group,
                            'timepoints': {}
                        }
                    self.data_by_subject[subject_id]['timepoints'][timepoint] = file_info
        
        # Print summary
        print("\nðŸ“Š Data Organization Summary:")
        for timepoint in self.timepoints:
            n_patients = len(self.data_by_timepoint[timepoint]['patients'])
            n_controls = len(self.data_by_timepoint[timepoint]['controls'])
            print(f"  {timepoint}: {n_patients} patients, {n_controls} controls")
        
        print(f"\nðŸ‘¥ Longitudinal Subject Summary:")
        patients_with_all_timepoints = 0
        controls_with_all_timepoints = 0
        
        for subject_id, subject_data in self.data_by_subject.items():
            timepoints_available = len(subject_data['timepoints'])
            if timepoints_available == 3:  # All timepoints available
                if subject_data['group'] == 'patients':
                    patients_with_all_timepoints += 1
                else:
                    controls_with_all_timepoints += 1
        
        print(f"  Patients with all 3 timepoints: {patients_with_all_timepoints}")
        print(f"  Controls with all 3 timepoints: {controls_with_all_timepoints}")
        
        return self.data_by_timepoint, self.data_by_subject
    
    def create_brain_masker(self):
        """Create optimized brain masker for signal extraction"""
        print("\nðŸ§  Creating brain masker...")
        
        # Load a sample image to determine dimensions
        sample_file = None
        for timepoint in self.timepoints:
            if self.data_by_timepoint[timepoint]['patients']:
                sample_file = self.data_by_timepoint[timepoint]['patients'][0]['file_path']
                break
        
        if not sample_file:
            raise ValueError("No sample file found for masker creation")
        
        sample_img = nib.load(sample_file)
        
        # Create masker optimized for SCA analysis
        self.masker = NiftiMasker(
            mask_strategy='background',
            standardize=True,
            memory='nilearn_cache',
            memory_level=1,
            smoothing_fwhm=6,  # Smooth for better signal-to-noise ratio
            verbose=0
        )
        
        # Fit masker on sample image
        self.masker.fit(sample_img)
        
        n_voxels = self.masker.mask_img_.get_fdata().sum()
        print(f"âœ… Brain masker created with {n_voxels:.0f} voxels")
        
        return self.masker
    
    def extract_signals_by_timepoint(self):
        """Extract brain signals for each timepoint separately"""
        print("\nðŸ“¡ Extracting brain signals by timepoint...")
        
        timepoint_signals = {}
        
        for timepoint in tqdm(self.timepoints, desc="Processing timepoints"):
            print(f"\n  Processing {timepoint}...")
            
            all_signals = []
            all_labels = []
            all_metadata = []
            
            # Process patients
            for file_info in tqdm(self.data_by_timepoint[timepoint]['patients'], 
                                desc=f"{timepoint} patients", leave=False):
                try:
                    img = nib.load(file_info['file_path'])
                    signals = self.masker.transform(img)
                    mean_signal = signals.mean(axis=0)  # Average across time
                    
                    all_signals.append(mean_signal)
                    all_labels.append(1)  # Patient = 1
                    all_metadata.append(file_info)
                    
                except Exception as e:
                    print(f"    Error with {file_info['filename']}: {e}")
                    continue
            
            # Process controls
            for file_info in tqdm(self.data_by_timepoint[timepoint]['controls'], 
                                desc=f"{timepoint} controls", leave=False):
                try:
                    img = nib.load(file_info['file_path'])
                    signals = self.masker.transform(img)
                    mean_signal = signals.mean(axis=0)  # Average across time
                    
                    all_signals.append(mean_signal)
                    all_labels.append(0)  # Control = 0
                    all_metadata.append(file_info)
                    
                except Exception as e:
                    print(f"    Error with {file_info['filename']}: {e}")
                    continue
            
            # Store timepoint data
            if all_signals:
                timepoint_signals[timepoint] = {
                    'signals': np.array(all_signals),
                    'labels': np.array(all_labels),
                    'metadata': all_metadata
                }
                
                n_patients = np.sum(timepoint_signals[timepoint]['labels'] == 1)
                n_controls = np.sum(timepoint_signals[timepoint]['labels'] == 0)
                print(f"    âœ… {timepoint}: {n_patients} patients, {n_controls} controls processed")
        
        self.timepoint_signals = timepoint_signals
        return timepoint_signals
    
    def perform_ica_by_timepoint(self):
        """Perform ICA analysis for each timepoint to identify brain networks"""
        print(f"\nðŸ”¬ Performing ICA analysis with {self.n_components} components...")
        
        self.ica_results = {}
        
        for timepoint in tqdm(self.timepoints, desc="ICA analysis"):
            if timepoint not in self.timepoint_signals:
                continue
                
            print(f"\n  ICA for {timepoint}...")
            signals = self.timepoint_signals[timepoint]['signals']
            
            # Standardize signals
            scaler = RobustScaler()  # More robust to outliers
            signals_scaled = scaler.fit_transform(signals)
            
            # Apply PCA first for dimensionality reduction
            # Ensure n_components doesn't exceed min(n_samples, n_features)
            max_components = min(signals.shape[0]-1, signals.shape[1]//10, 20)
            pca = PCA(n_components=max_components, random_state=42)
            signals_pca = pca.fit_transform(signals_scaled)
            
            # Apply FastICA
            ica = FastICA(
                n_components=self.n_components,
                random_state=42,
                max_iter=1000,
                tol=1e-3,
                algorithm='parallel'
            )
            
            ica_components = ica.fit_transform(signals_pca)
            
            # Store results
            self.ica_results[timepoint] = {
                'components': ica_components,
                'spatial_maps': ica.components_,
                'labels': self.timepoint_signals[timepoint]['labels'],
                'metadata': self.timepoint_signals[timepoint]['metadata'],
                'scaler': scaler,
                'pca': pca,
                'ica': ica,
                'explained_variance': pca.explained_variance_ratio_.sum()
            }
            
            print(f"    âœ… {timepoint}: {ica_components.shape} components extracted")
            print(f"    ðŸ“Š PCA explained variance: {pca.explained_variance_ratio_.sum():.3f}")
        
        return self.ica_results
    
    def identify_compensation_patterns(self):
        """Identify which networks show increased activity (compensation) in patients"""
        print("\nðŸ” Identifying compensation patterns...")
        
        self.compensation_analysis = {}
        
        for timepoint in self.timepoints:
            if timepoint not in self.ica_results:
                continue
                
            print(f"\n  Analyzing compensation in {timepoint}...")
            
            components = self.ica_results[timepoint]['components']
            labels = self.ica_results[timepoint]['labels']
            
            compensation_patterns = []
            
            for comp_idx in range(components.shape[1]):
                patient_values = components[labels == 1, comp_idx]
                control_values = components[labels == 0, comp_idx]
                
                if len(patient_values) > 0 and len(control_values) > 0:
                    # Statistical tests
                    t_stat, t_pval = ttest_ind(patient_values, control_values)
                    u_stat, u_pval = mannwhitneyu(patient_values, control_values, alternative='two-sided')
                    
                    # Effect size (Cohen's d)
                    pooled_std = np.sqrt(((len(patient_values)-1)*np.var(patient_values, ddof=1) + 
                                        (len(control_values)-1)*np.var(control_values, ddof=1)) / 
                                       (len(patient_values) + len(control_values) - 2))
                    cohens_d = (np.mean(patient_values) - np.mean(control_values)) / pooled_std
                    
                    # Determine if this represents compensation (increased activity in patients)
                    is_compensation = np.mean(patient_values) > np.mean(control_values)
                    compensation_strength = cohens_d if is_compensation else -cohens_d
                    
                    compensation_patterns.append({
                        'component': comp_idx,
                        'timepoint': timepoint,
                        'patient_mean': np.mean(patient_values),
                        'control_mean': np.mean(control_values),
                        'patient_std': np.std(patient_values),
                        'control_std': np.std(control_values),
                        'cohens_d': cohens_d,
                        'compensation_strength': compensation_strength,
                        'is_compensation': is_compensation,
                        't_pvalue': t_pval,
                        'u_pvalue': u_pval,
                        'n_patients': len(patient_values),
                        'n_controls': len(control_values)
                    })
            
            self.compensation_analysis[timepoint] = compensation_patterns
            
            # Count compensation networks
            n_compensation = sum(1 for p in compensation_patterns if p['is_compensation'] and p['u_pvalue'] < 0.05)
            print(f"    ðŸ“ˆ Found {n_compensation} significant compensation networks in {timepoint}")
        
        return self.compensation_analysis
    
    def create_personalized_compensation_maps(self):
        """Create individual compensation maps for each subject across timepoints"""
        print("\nðŸ—ºï¸ Creating personalized compensation maps...")
        
        self.compensation_maps = {}
        
        # Only analyze subjects with complete longitudinal data
        complete_subjects = []
        for subject_id, subject_data in self.data_by_subject.items():
            if len(subject_data['timepoints']) == 3:  # All timepoints available
                complete_subjects.append(subject_id)
        
        print(f"ðŸ“Š Analyzing {len(complete_subjects)} subjects with complete data")
        
        for subject_id in tqdm(complete_subjects, desc="Creating compensation maps"):
            subject_data = self.data_by_subject[subject_id]
            group = subject_data['group']
            
            if group != 'patients':  # Focus on patient compensation
                continue
            
            compensation_map = {
                'subject_id': subject_id,
                'group': group,
                'timepoint_patterns': {},
                'compensation_trends': {},
                'compensation_score': {}
            }
            
            # Extract individual network strengths across timepoints
            for timepoint in self.timepoints:
                if (timepoint in subject_data['timepoints'] and 
                    timepoint in self.ica_results):
                    
                    # Find this subject's data in the ICA results
                    metadata = self.ica_results[timepoint]['metadata']
                    components = self.ica_results[timepoint]['components']
                    
                    subject_idx = None
                    for idx, meta in enumerate(metadata):
                        if meta['subject_id'] == subject_id:
                            subject_idx = idx
                            break
                    
                    if subject_idx is not None:
                        subject_components = components[subject_idx, :]
                        compensation_map['timepoint_patterns'][timepoint] = subject_components.tolist()
                        
                        # Calculate compensation score (how much higher than control mean)
                        compensation_scores = []
                        for comp_idx in range(len(subject_components)):
                            comp_analysis = self.compensation_analysis[timepoint][comp_idx]
                            if comp_analysis['is_compensation']:
                                relative_strength = (subject_components[comp_idx] - comp_analysis['control_mean']) / comp_analysis['control_std']
                                compensation_scores.append(max(0, relative_strength))  # Only positive compensation
                        
                        compensation_map['compensation_score'][timepoint] = np.mean(compensation_scores) if compensation_scores else 0
            
            # Calculate compensation trends (how compensation changes over time)
            if len(compensation_map['compensation_score']) >= 2:
                timepoints_with_data = sorted(compensation_map['compensation_score'].keys())
                scores = [compensation_map['compensation_score'][tp] for tp in timepoints_with_data]
                
                # Linear trend
                if len(scores) >= 2:
                    x = np.arange(len(scores))
                    slope, intercept, r_value, p_value, std_err = stats.linregress(x, scores)
                    compensation_map['compensation_trends'] = {
                        'slope': slope,
                        'r_squared': r_value**2,
                        'p_value': p_value,
                        'trend_direction': 'increasing' if slope > 0 else 'decreasing',
                        'trend_strength': abs(slope)
                    }
            
            self.compensation_maps[subject_id] = compensation_map
        
        print(f"âœ… Created compensation maps for {len(self.compensation_maps)} patients")
        return self.compensation_maps
    
    def identify_early_biomarkers(self):
        """Identify rs-1 patterns that predict rs-2 and rs-3 outcomes"""
        print("\nðŸŽ¯ Identifying early biomarkers (rs-1 â†’ rs-2/rs-3 prediction)...")
        
        # Prepare prediction dataset
        prediction_data = []
        
        for subject_id, comp_map in self.compensation_maps.items():
            if ('rs-1' in comp_map['timepoint_patterns'] and 
                'rs-2' in comp_map['timepoint_patterns'] and
                'rs-3' in comp_map['timepoint_patterns']):
                
                rs1_patterns = np.array(comp_map['timepoint_patterns']['rs-1'])
                rs2_patterns = np.array(comp_map['timepoint_patterns']['rs-2'])
                rs3_patterns = np.array(comp_map['timepoint_patterns']['rs-3'])
                
                prediction_data.append({
                    'subject_id': subject_id,
                    'rs1_features': rs1_patterns,
                    'rs2_targets': rs2_patterns,
                    'rs3_targets': rs3_patterns,
                    'rs1_compensation_score': comp_map['compensation_score'].get('rs-1', 0),
                    'rs2_compensation_score': comp_map['compensation_score'].get('rs-2', 0),
                    'rs3_compensation_score': comp_map['compensation_score'].get('rs-3', 0)
                })
        
        if len(prediction_data) < 5:
            print("âŒ Insufficient data for prediction analysis")
            return None
        
        print(f"ðŸ“Š Using {len(prediction_data)} subjects for prediction analysis")
        
        # Prepare matrices
        X_rs1 = np.array([data['rs1_features'] for data in prediction_data])  # rs-1 features
        y_rs2 = np.array([data['rs2_targets'] for data in prediction_data])  # rs-2 targets
        y_rs3 = np.array([data['rs3_targets'] for data in prediction_data])  # rs-3 targets
        
        # Predict rs-2 from rs-1
        print("\n  ðŸ”® Predicting rs-2 patterns from rs-1...")
        self.rs1_to_rs2_prediction = self._train_prediction_model(X_rs1, y_rs2, "rs-1 â†’ rs-2")
        
        # Predict rs-3 from rs-1
        print("\n  ðŸ”® Predicting rs-3 patterns from rs-1...")
        self.rs1_to_rs3_prediction = self._train_prediction_model(X_rs1, y_rs3, "rs-1 â†’ rs-3")
        
        # Predict compensation scores
        print("\n  ðŸ“ˆ Predicting compensation score changes...")
        y_comp_change_rs2 = np.array([data['rs2_compensation_score'] - data['rs1_compensation_score'] for data in prediction_data])
        y_comp_change_rs3 = np.array([data['rs3_compensation_score'] - data['rs1_compensation_score'] for data in prediction_data])
        
        self.compensation_score_prediction = {
            'rs1_to_rs2_change': self._train_compensation_prediction(X_rs1, y_comp_change_rs2, "rs-1 â†’ rs-2 compensation change"),
            'rs1_to_rs3_change': self._train_compensation_prediction(X_rs1, y_comp_change_rs3, "rs-1 â†’ rs-3 compensation change")
        }
        
        return {
            'pattern_prediction': {
                'rs1_to_rs2': self.rs1_to_rs2_prediction,
                'rs1_to_rs3': self.rs1_to_rs3_prediction
            },
            'compensation_prediction': self.compensation_score_prediction
        }
    
    def _train_prediction_model(self, X, y, description):
        """Train a model to predict future network patterns"""
        if len(X) < 5:
            return None
        
        # Use Random Forest optimized for small sample size
        model = RandomForestRegressor(
            n_estimators=50,  # Reduced for small sample
            max_depth=3,      # Shallow to prevent overfitting
            min_samples_split=3,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        # Cross-validation for each component
        component_predictions = {}
        
        for comp_idx in range(y.shape[1]):
            y_comp = y[:, comp_idx]
            
            # Cross-validation - adjust CV folds for small sample size
            cv_folds = min(3, len(X)//2) if len(X) >= 4 else 2
            cv_scores = cross_val_score(model, X, y_comp, cv=cv_folds, scoring='r2', n_jobs=-1)
            
            # Train on full data for feature importance
            model.fit(X, y_comp)
            
            component_predictions[comp_idx] = {
                'cv_r2_mean': np.mean(cv_scores),
                'cv_r2_std': np.std(cv_scores),
                'feature_importance': model.feature_importances_.tolist(),
                'prediction_quality': 'good' if np.mean(cv_scores) > 0.3 else 'poor'
            }
        
        # Overall prediction quality
        avg_r2 = np.mean([comp['cv_r2_mean'] for comp in component_predictions.values()])
        print(f"    ðŸ“Š {description}: Average RÂ² = {avg_r2:.3f}")
        
        return {
            'description': description,
            'avg_r2': avg_r2,
            'component_predictions': component_predictions,
            'n_subjects': len(X)
        }
    
    def _train_compensation_prediction(self, X, y, description):
        """Train model to predict compensation score changes"""
        if len(X) < 5:
            return None
        
        model = RandomForestRegressor(
            n_estimators=100,
            max_depth=8,
            random_state=42,
            n_jobs=-1
        )
        
        # Cross-validation - adjust CV folds for small sample size
        cv_folds = min(3, len(X)//2) if len(X) >= 4 else 2
        cv_scores = cross_val_score(model, X, y, cv=cv_folds, scoring='r2', n_jobs=-1)
        
        # Train on full data
        model.fit(X, y)
        
        print(f"    ðŸ“ˆ {description}: RÂ² = {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}")
        
        return {
            'description': description,
            'cv_r2_mean': np.mean(cv_scores),
            'cv_r2_std': np.std(cv_scores),
            'feature_importance': model.feature_importances_.tolist(),
            'model': model
        }
    
    def save_comprehensive_results(self):
        """Save all analysis results"""
        print("\nðŸ’¾ Saving comprehensive results...")
        
        # Create main results dictionary
        results = {
            'analysis_info': {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'n_components': self.n_components,
                'timepoints': self.timepoints,
                'n_subjects_analyzed': len(self.compensation_maps)
            },
            'compensation_analysis': self.compensation_analysis,
            'compensation_maps': self.compensation_maps,
            'early_biomarker_predictions': {
                'rs1_to_rs2': self.rs1_to_rs2_prediction if hasattr(self, 'rs1_to_rs2_prediction') else None,
                'rs1_to_rs3': self.rs1_to_rs3_prediction if hasattr(self, 'rs1_to_rs3_prediction') else None,
                'compensation_scores': self.compensation_score_prediction if hasattr(self, 'compensation_score_prediction') else None
            }
        }
        
        # Save main results
        results_file = self.output_dir / 'longitudinal_ica_compensation_results.json'
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=4, default=str)
        
        # Save individual component data as CSV
        for timepoint in self.timepoints:
            if timepoint in self.ica_results:
                components_df = pd.DataFrame(
                    self.ica_results[timepoint]['components'],
                    columns=[f'ICA_Component_{i+1}' for i in range(self.n_components)]
                )
                # Add metadata
                for idx, meta in enumerate(self.ica_results[timepoint]['metadata']):
                    components_df.loc[idx, 'Subject_ID'] = meta['subject_id']
                    components_df.loc[idx, 'Group'] = meta['group']
                    components_df.loc[idx, 'Timepoint'] = timepoint
                
                csv_file = self.output_dir / f'ica_components_{timepoint}.csv'
                components_df.to_csv(csv_file, index=False)
        
        # Save compensation summary
        compensation_summary = []
        for subject_id, comp_map in self.compensation_maps.items():
            summary_row = {
                'Subject_ID': subject_id,
                'Group': comp_map['group']
            }
            
            # Add compensation scores
            for tp in self.timepoints:
                summary_row[f'Compensation_Score_{tp}'] = comp_map['compensation_score'].get(tp, np.nan)
            
            # Add trend information
            if 'compensation_trends' in comp_map:
                summary_row.update({
                    'Trend_Slope': comp_map['compensation_trends'].get('slope', np.nan),
                    'Trend_R_Squared': comp_map['compensation_trends'].get('r_squared', np.nan),
                    'Trend_Direction': comp_map['compensation_trends'].get('trend_direction', 'unknown')
                })
            
            compensation_summary.append(summary_row)
        
        compensation_df = pd.DataFrame(compensation_summary)
        compensation_file = self.output_dir / 'compensation_summary.csv'
        compensation_df.to_csv(compensation_file, index=False)
        
        print(f"âœ… Results saved to: {self.output_dir}")
        print(f"   ðŸ“„ Main results: {results_file}")
        print(f"   ðŸ“Š Compensation summary: {compensation_file}")
        
        return results_file
    
    def create_visualizations(self):
        """Create comprehensive visualizations"""
        print("\nðŸŽ¨ Creating visualizations...")
        
        # Set up plotting parameters
        plt.rcParams['figure.figsize'] = (12, 8)
        plt.rcParams['font.size'] = 10
        
        # 1. Compensation patterns across timepoints
        self._plot_compensation_patterns()
        
        # 2. Individual compensation maps
        self._plot_individual_compensation_maps()
        
        # 3. Early biomarker prediction performance
        self._plot_prediction_performance()
        
        # 4. Compensation score trends
        self._plot_compensation_trends()
        
        print("âœ… All visualizations created")
    
    def _plot_compensation_patterns(self):
        """Plot compensation patterns across timepoints"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Brain Compensation Patterns Across Timepoints', fontsize=16, fontweight='bold')
        
        # Plot 1: Number of compensation networks per timepoint
        ax1 = axes[0, 0]
        timepoint_counts = []
        for tp in self.timepoints:
            if tp in self.compensation_analysis:
                n_comp = sum(1 for p in self.compensation_analysis[tp] 
                           if p['is_compensation'] and p['u_pvalue'] < 0.05)
                timepoint_counts.append(n_comp)
            else:
                timepoint_counts.append(0)
        
        bars = ax1.bar(self.timepoints, timepoint_counts, color=['#3498db', '#e74c3c', '#2ecc71'])
        ax1.set_title('Number of Significant Compensation Networks')
        ax1.set_ylabel('Number of Networks')
        ax1.set_xlabel('Timepoint')
        
        # Add value labels on bars
        for bar, count in zip(bars, timepoint_counts):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    str(count), ha='center', va='bottom', fontweight='bold')
        
        # Plot 2: Average compensation strength across timepoints
        ax2 = axes[0, 1]
        avg_strengths = []
        for tp in self.timepoints:
            if tp in self.compensation_analysis:
                strengths = [p['compensation_strength'] for p in self.compensation_analysis[tp] 
                           if p['is_compensation'] and p['u_pvalue'] < 0.05]
                avg_strengths.append(np.mean(strengths) if strengths else 0)
            else:
                avg_strengths.append(0)
        
        ax2.plot(self.timepoints, avg_strengths, marker='o', linewidth=3, markersize=8, color='#e74c3c')
        ax2.set_title('Average Compensation Strength Over Time')
        ax2.set_ylabel('Average Effect Size (Cohen\'s d)')
        ax2.set_xlabel('Timepoint')
        ax2.grid(True, alpha=0.3)
        
        # Plot 3: Compensation score distribution
        ax3 = axes[1, 0]
        all_scores = []
        all_timepoints = []
        
        for subject_id, comp_map in self.compensation_maps.items():
            for tp, score in comp_map['compensation_score'].items():
                all_scores.append(score)
                all_timepoints.append(tp)
        
        if all_scores:
            # Create box plot
            score_data = []
            for tp in self.timepoints:
                tp_scores = [score for score, timepoint in zip(all_scores, all_timepoints) if timepoint == tp]
                score_data.append(tp_scores)
            
            box_plot = ax3.boxplot(score_data, labels=self.timepoints, patch_artist=True)
            colors = ['#3498db', '#e74c3c', '#2ecc71']
            for patch, color in zip(box_plot['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
            
            ax3.set_title('Distribution of Individual Compensation Scores')
            ax3.set_ylabel('Compensation Score')
            ax3.set_xlabel('Timepoint')
        
        # Plot 4: Prediction performance summary
        ax4 = axes[1, 1]
        if hasattr(self, 'rs1_to_rs2_prediction') and self.rs1_to_rs2_prediction:
            r2_rs2 = self.rs1_to_rs2_prediction['avg_r2']
            r2_rs3 = self.rs1_to_rs3_prediction['avg_r2'] if hasattr(self, 'rs1_to_rs3_prediction') and self.rs1_to_rs3_prediction else 0
            
            predictions = ['rs-1 â†’ rs-2', 'rs-1 â†’ rs-3']
            r2_values = [r2_rs2, r2_rs3]
            
            bars = ax4.bar(predictions, r2_values, color=['#f39c12', '#9b59b6'])
            ax4.set_title('Early Biomarker Prediction Performance')
            ax4.set_ylabel('RÂ² Score')
            ax4.set_ylim(0, 1)
            ax4.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='Good prediction threshold')
            ax4.legend()
            
            # Add value labels
            for bar, r2 in zip(bars, r2_values):
                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                        f'{r2:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'compensation_patterns_overview.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_individual_compensation_maps(self):
        """Plot individual compensation maps for selected patients"""
        # Select a few representative patients
        representative_patients = list(self.compensation_maps.keys())[:6]  # First 6 patients
        
        if not representative_patients:
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Individual Patient Compensation Maps', fontsize=16, fontweight='bold')
        
        for idx, subject_id in enumerate(representative_patients):
            if idx >= 6:
                break
                
            row = idx // 3
            col = idx % 3
            ax = axes[row, col]
            
            comp_map = self.compensation_maps[subject_id]
            
            # Plot compensation scores across timepoints
            timepoints_with_data = []
            scores = []
            
            for tp in self.timepoints:
                if tp in comp_map['compensation_score']:
                    timepoints_with_data.append(tp)
                    scores.append(comp_map['compensation_score'][tp])
            
            if len(scores) >= 2:
                ax.plot(timepoints_with_data, scores, marker='o', linewidth=2, markersize=6)
                ax.set_title(f'Patient {subject_id}', fontweight='bold')
                ax.set_ylabel('Compensation Score')
                ax.set_xlabel('Timepoint')
                ax.grid(True, alpha=0.3)
                
                # Add trend information if available
                if 'compensation_trends' in comp_map and comp_map['compensation_trends']:
                    trend_info = comp_map['compensation_trends']
                    direction = trend_info['trend_direction']
                    r2 = trend_info['r_squared']
                    ax.text(0.05, 0.95, f'Trend: {direction}\nRÂ² = {r2:.3f}', 
                           transform=ax.transAxes, va='top', 
                           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            else:
                ax.text(0.5, 0.5, 'Insufficient\nData', ha='center', va='center', 
                       transform=ax.transAxes, fontsize=12, alpha=0.5)
                ax.set_title(f'Patient {subject_id}', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'individual_compensation_maps.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_prediction_performance(self):
        """Plot prediction performance details"""
        if not (hasattr(self, 'rs1_to_rs2_prediction') and self.rs1_to_rs2_prediction):
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Early Biomarker Prediction Performance', fontsize=16, fontweight='bold')
        
        # Plot 1: Component-wise prediction performance for rs-1 â†’ rs-2
        ax1 = axes[0, 0]
        comp_predictions = self.rs1_to_rs2_prediction['component_predictions']
        component_indices = list(comp_predictions.keys())
        r2_scores = [comp_predictions[idx]['cv_r2_mean'] for idx in component_indices]
        
        bars = ax1.bar(component_indices, r2_scores, alpha=0.7)
        ax1.set_title('rs-1 â†’ rs-2: Component Prediction Performance')
        ax1.set_xlabel('ICA Component')
        ax1.set_ylabel('Cross-validation RÂ²')
        ax1.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='Good prediction threshold')
        ax1.legend()
        
        # Color bars based on performance
        for bar, r2 in zip(bars, r2_scores):
            if r2 > 0.3:
                bar.set_color('#2ecc71')  # Good performance
            elif r2 > 0.1:
                bar.set_color('#f39c12')  # Moderate performance
            else:
                bar.set_color('#e74c3c')  # Poor performance
        
        # Plot 2: Component-wise prediction performance for rs-1 â†’ rs-3
        if hasattr(self, 'rs1_to_rs3_prediction') and self.rs1_to_rs3_prediction:
            ax2 = axes[0, 1]
            comp_predictions_rs3 = self.rs1_to_rs3_prediction['component_predictions']
            r2_scores_rs3 = [comp_predictions_rs3[idx]['cv_r2_mean'] for idx in component_indices]
            
            bars = ax2.bar(component_indices, r2_scores_rs3, alpha=0.7)
            ax2.set_title('rs-1 â†’ rs-3: Component Prediction Performance')
            ax2.set_xlabel('ICA Component')
            ax2.set_ylabel('Cross-validation RÂ²')
            ax2.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='Good prediction threshold')
            ax2.legend()
            
            # Color bars based on performance
            for bar, r2 in zip(bars, r2_scores_rs3):
                if r2 > 0.3:
                    bar.set_color('#2ecc71')
                elif r2 > 0.1:
                    bar.set_color('#f39c12')
                else:
                    bar.set_color('#e74c3c')
        
        # Plot 3: Comparison of prediction horizons
        ax3 = axes[1, 0]
        if hasattr(self, 'rs1_to_rs3_prediction') and self.rs1_to_rs3_prediction:
            prediction_comparison = pd.DataFrame({
                'rs-1 â†’ rs-2': r2_scores,
                'rs-1 â†’ rs-3': r2_scores_rs3
            })
            
            prediction_comparison.boxplot(ax=ax3)
            ax3.set_title('Prediction Performance Comparison')
            ax3.set_ylabel('RÂ² Score')
            ax3.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='Good prediction threshold')
            ax3.legend()
        
        # Plot 4: Compensation score prediction performance
        ax4 = axes[1, 1]
        if hasattr(self, 'compensation_score_prediction') and self.compensation_score_prediction:
            comp_pred = self.compensation_score_prediction
            
            predictions = []
            r2_values = []
            
            if comp_pred['rs1_to_rs2_change']:
                predictions.append('rs-1 â†’ rs-2\nComp. Change')
                r2_values.append(comp_pred['rs1_to_rs2_change']['cv_r2_mean'])
            
            if comp_pred['rs1_to_rs3_change']:
                predictions.append('rs-1 â†’ rs-3\nComp. Change')
                r2_values.append(comp_pred['rs1_to_rs3_change']['cv_r2_mean'])
            
            if predictions:
                bars = ax4.bar(predictions, r2_values, color=['#3498db', '#e74c3c'])
                ax4.set_title('Compensation Score Change Prediction')
                ax4.set_ylabel('RÂ² Score')
                ax4.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='Good prediction threshold')
                ax4.legend()
                
                # Add value labels
                for bar, r2 in zip(bars, r2_values):
                    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                            f'{r2:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'prediction_performance_details.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_compensation_trends(self):
        """Plot compensation trends across all patients"""
        if not self.compensation_maps:
            return
        
        # Prepare data for trend analysis
        trend_data = []
        for subject_id, comp_map in self.compensation_maps.items():
            if 'compensation_trends' in comp_map and comp_map['compensation_trends']:
                trend_info = comp_map['compensation_trends']
                trend_data.append({
                    'Subject_ID': subject_id,
                    'Slope': trend_info['slope'],
                    'R_Squared': trend_info['r_squared'],
                    'Direction': trend_info['trend_direction'],
                    'Strength': trend_info['trend_strength']
                })
        
        if not trend_data:
            return
        
        trend_df = pd.DataFrame(trend_data)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Compensation Trend Analysis Across All Patients', fontsize=16, fontweight='bold')
        
        # Plot 1: Distribution of trend slopes
        ax1 = axes[0, 0]
        ax1.hist(trend_df['Slope'], bins=10, alpha=0.7, color='#3498db')
        ax1.set_title('Distribution of Compensation Trend Slopes')
        ax1.set_xlabel('Slope (Compensation Change Rate)')
        ax1.set_ylabel('Number of Patients')
        ax1.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='No change')
        ax1.legend()
        
        # Plot 2: Trend direction pie chart
        ax2 = axes[0, 1]
        direction_counts = trend_df['Direction'].value_counts()
        colors = ['#e74c3c' if dir == 'decreasing' else '#2ecc71' for dir in direction_counts.index]
        ax2.pie(direction_counts.values, labels=direction_counts.index, autopct='%1.1f%%', 
               colors=colors, startangle=90)
        ax2.set_title('Compensation Trend Directions')
        
        # Plot 3: Slope vs R-squared (trend quality)
        ax3 = axes[1, 0]
        scatter = ax3.scatter(trend_df['Slope'], trend_df['R_Squared'], 
                             c=trend_df['Strength'], cmap='viridis', alpha=0.7, s=60)
        ax3.set_xlabel('Trend Slope')
        ax3.set_ylabel('Trend RÂ² (Quality)')
        ax3.set_title('Trend Quality vs. Compensation Change Rate')
        ax3.axvline(x=0, color='red', linestyle='--', alpha=0.5)
        ax3.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Good trend threshold')
        ax3.legend()
        
        # Add colorbar
        cbar = plt.colorbar(scatter, ax=ax3)
        cbar.set_label('Trend Strength')
        
        # Plot 4: Individual patient trends (top 8 by trend quality)
        ax4 = axes[1, 1]
        top_patients = trend_df.nlargest(8, 'R_Squared')
        
        for idx, (_, patient_data) in enumerate(top_patients.iterrows()):
            subject_id = patient_data['Subject_ID']
            comp_map = self.compensation_maps[subject_id]
            
            timepoints_with_data = []
            scores = []
            for tp in self.timepoints:
                if tp in comp_map['compensation_score']:
                    timepoints_with_data.append(tp)
                    scores.append(comp_map['compensation_score'][tp])
            
            if len(scores) >= 2:
                ax4.plot(timepoints_with_data, scores, marker='o', alpha=0.7, 
                        label=f'{subject_id} (RÂ²={patient_data["R_Squared"]:.2f})')
        
        ax4.set_title('Top 8 Patients by Trend Quality')
        ax4.set_xlabel('Timepoint')
        ax4.set_ylabel('Compensation Score')
        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'compensation_trends_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_comprehensive_report(self):
        """Generate a comprehensive analysis report"""
        print("\nðŸ“‹ Generating comprehensive report...")
        
        report_lines = []
        report_lines.append("# Longitudinal ICA Compensation Analysis Report")
        report_lines.append("=" * 60)
        report_lines.append(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # Analysis Overview
        report_lines.append("## Analysis Overview")
        report_lines.append(f"- **Objective**: Map individual SCA brain compensation strategies over time")
        report_lines.append(f"- **Method**: Independent Component Analysis (ICA) + Longitudinal prediction")
        report_lines.append(f"- **Timepoints**: {', '.join(self.timepoints)}")
        report_lines.append(f"- **ICA Components**: {self.n_components}")
        report_lines.append(f"- **Patients Analyzed**: {len(self.compensation_maps)}")
        report_lines.append("")
        
        # Data Summary
        report_lines.append("## Data Summary")
        for timepoint in self.timepoints:
            if timepoint in self.timepoint_signals:
                signals = self.timepoint_signals[timepoint]
                n_patients = np.sum(signals['labels'] == 1)
                n_controls = np.sum(signals['labels'] == 0)
                report_lines.append(f"- **{timepoint}**: {n_patients} patients, {n_controls} controls")
        report_lines.append("")
        
        # Compensation Analysis Results
        report_lines.append("## Compensation Network Analysis")
        for timepoint in self.timepoints:
            if timepoint in self.compensation_analysis:
                comp_networks = [p for p in self.compensation_analysis[timepoint] 
                               if p['is_compensation'] and p['u_pvalue'] < 0.05]
                
                report_lines.append(f"### {timepoint}")
                report_lines.append(f"- **Significant compensation networks**: {len(comp_networks)}")
                
                if comp_networks:
                    avg_effect = np.mean([p['compensation_strength'] for p in comp_networks])
                    report_lines.append(f"- **Average compensation effect size**: {avg_effect:.3f}")
                    
                    # Top 3 compensation networks
                    top_networks = sorted(comp_networks, 
                                        key=lambda x: x['compensation_strength'], reverse=True)[:3]
                    report_lines.append("- **Top compensation networks**:")
                    for i, network in enumerate(top_networks, 1):
                        comp_idx = network['component']
                        effect_size = network['compensation_strength']
                        p_val = network['u_pvalue']
                        report_lines.append(f"  {i}. Component {comp_idx}: Effect size = {effect_size:.3f}, p = {p_val:.4f}")
                
                report_lines.append("")
        
        # Early Biomarker Prediction Results
        report_lines.append("## Early Biomarker Prediction Performance")
        
        if hasattr(self, 'rs1_to_rs2_prediction') and self.rs1_to_rs2_prediction:
            r2_rs2 = self.rs1_to_rs2_prediction['avg_r2']
            report_lines.append(f"- **rs-1 â†’ rs-2 prediction**: RÂ² = {r2_rs2:.3f}")
            
            # Count good predictions
            good_predictions_rs2 = sum(1 for comp_pred in self.rs1_to_rs2_prediction['component_predictions'].values()
                                     if comp_pred['cv_r2_mean'] > 0.3)
            total_components = len(self.rs1_to_rs2_prediction['component_predictions'])
            report_lines.append(f"  - Components with good prediction (RÂ² > 0.3): {good_predictions_rs2}/{total_components}")
        
        if hasattr(self, 'rs1_to_rs3_prediction') and self.rs1_to_rs3_prediction:
            r2_rs3 = self.rs1_to_rs3_prediction['avg_r2']
            report_lines.append(f"- **rs-1 â†’ rs-3 prediction**: RÂ² = {r2_rs3:.3f}")
            
            good_predictions_rs3 = sum(1 for comp_pred in self.rs1_to_rs3_prediction['component_predictions'].values()
                                     if comp_pred['cv_r2_mean'] > 0.3)
            total_components = len(self.rs1_to_rs3_prediction['component_predictions'])
            report_lines.append(f"  - Components with good prediction (RÂ² > 0.3): {good_predictions_rs3}/{total_components}")
        
        if hasattr(self, 'compensation_score_prediction') and self.compensation_score_prediction:
            report_lines.append("- **Compensation score change prediction**:")
            if self.compensation_score_prediction['rs1_to_rs2_change']:
                r2_comp_rs2 = self.compensation_score_prediction['rs1_to_rs2_change']['cv_r2_mean']
                report_lines.append(f"  - rs-1 â†’ rs-2 compensation change: RÂ² = {r2_comp_rs2:.3f}")
            if self.compensation_score_prediction['rs1_to_rs3_change']:
                r2_comp_rs3 = self.compensation_score_prediction['rs1_to_rs3_change']['cv_r2_mean']
                report_lines.append(f"  - rs-1 â†’ rs-3 compensation change: RÂ² = {r2_comp_rs3:.3f}")
        
        report_lines.append("")
        
        # Individual Compensation Trends
        report_lines.append("## Individual Compensation Trends")
        
        if self.compensation_maps:
            # Calculate trend statistics
            trends_with_data = [comp_map for comp_map in self.compensation_maps.values() 
                              if 'compensation_trends' in comp_map and comp_map['compensation_trends']]
            
            if trends_with_data:
                increasing_trends = sum(1 for t in trends_with_data 
                                      if t['compensation_trends']['trend_direction'] == 'increasing')
                decreasing_trends = len(trends_with_data) - increasing_trends
                
                avg_slope = np.mean([t['compensation_trends']['slope'] for t in trends_with_data])
                avg_r2 = np.mean([t['compensation_trends']['r_squared'] for t in trends_with_data])
                
                report_lines.append(f"- **Patients with trend data**: {len(trends_with_data)}")
                report_lines.append(f"- **Increasing compensation trends**: {increasing_trends}")
                report_lines.append(f"- **Decreasing compensation trends**: {decreasing_trends}")
                report_lines.append(f"- **Average trend slope**: {avg_slope:.4f}")
                report_lines.append(f"- **Average trend quality (RÂ²)**: {avg_r2:.3f}")
                
                # Identify patients with strongest compensation adaptation
                strong_adapters = [t for t in trends_with_data 
                                 if (t['compensation_trends']['trend_direction'] == 'increasing' and 
                                     t['compensation_trends']['r_squared'] > 0.5)]
                
                if strong_adapters:
                    report_lines.append(f"- **Strong adaptive compensators**: {len(strong_adapters)} patients")
                    report_lines.append("  - These patients show consistent increases in compensation over time")
        
        report_lines.append("")
        
        # Clinical Implications
        report_lines.append("## Clinical Implications")
        report_lines.append("### Early Biomarker Potential")
        
        # Assess early biomarker quality
        good_early_biomarkers = False
        if (hasattr(self, 'rs1_to_rs2_prediction') and self.rs1_to_rs2_prediction and 
            self.rs1_to_rs2_prediction['avg_r2'] > 0.3):
            good_early_biomarkers = True
            report_lines.append("- âœ… **Strong early biomarker potential detected**")
            report_lines.append("  - rs-1 compensation patterns can predict rs-2 outcomes with good accuracy")
        elif (hasattr(self, 'rs1_to_rs2_prediction') and self.rs1_to_rs2_prediction and 
              self.rs1_to_rs2_prediction['avg_r2'] > 0.1):
            report_lines.append("- âš ï¸ **Moderate early biomarker potential detected**")
            report_lines.append("  - rs-1 patterns show some predictive value but may need refinement")
        else:
            report_lines.append("- âŒ **Limited early biomarker potential with current approach**")
            report_lines.append("  - Consider alternative features or longer prediction horizons")
        
        report_lines.append("")
        report_lines.append("### Personalized Medicine Applications")
        if self.compensation_maps:
            report_lines.append("- **Individual compensation mapping**: Successfully created for each patient")
            report_lines.append("- **Personalized treatment targeting**: Possible based on compensation patterns")
            report_lines.append("- **Adaptive capacity assessment**: Can identify patients with strong vs. weak compensation")
        
        report_lines.append("")
        report_lines.append("### Future Research Directions")
        report_lines.append("- **Expand prediction horizon**: Test rs-1 prediction of 4th and 5th year outcomes")
        report_lines.append("- **Clinical correlation**: Link compensation patterns to symptom severity (SARA scores)")
        report_lines.append("- **Intervention targeting**: Use compensation maps to guide rehabilitation strategies")
        report_lines.append("- **Multi-site validation**: Test approach on independent patient cohorts")
        
        # Save report
        report_file = self.output_dir / 'compensation_analysis_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ… Comprehensive report saved to: {report_file}")
        return report_file
    
    def run_complete_analysis(self):
        """Run the complete longitudinal ICA compensation analysis"""
        start_time = time.time()
        
        print("ðŸš€ STARTING LONGITUDINAL ICA COMPENSATION ANALYSIS")
        print("=" * 60)
        
        try:
            # Step 1: Load and organize longitudinal data
            self.load_longitudinal_data()
            
            # Step 2: Create brain masker
            self.create_brain_masker()
            
            # Step 3: Extract signals by timepoint
            self.extract_signals_by_timepoint()
            
            # Step 4: Perform ICA analysis for each timepoint
            self.perform_ica_by_timepoint()
            
            # Step 5: Identify compensation patterns
            self.identify_compensation_patterns()
            
            # Step 6: Create personalized compensation maps
            self.create_personalized_compensation_maps()
            
            # Step 7: Identify early biomarkers
            early_biomarker_results = self.identify_early_biomarkers()
            
            # Step 8: Create visualizations
            self.create_visualizations()
            
            # Step 9: Save comprehensive results
            results_file = self.save_comprehensive_results()
            
            # Step 10: Generate comprehensive report
            report_file = self.generate_comprehensive_report()
            
            # Analysis complete
            end_time = time.time()
            total_time = end_time - start_time
            
            print("\nðŸŽ‰ LONGITUDINAL ICA COMPENSATION ANALYSIS COMPLETED!")
            print("=" * 60)
            print(f"â±ï¸  Total analysis time: {total_time/60:.1f} minutes")
            print(f"ðŸ‘¥ Patients analyzed: {len(self.compensation_maps)}")
            print(f"ðŸ§  Brain networks identified: {self.n_components}")
            print(f"ðŸ“Š Results saved to: {self.output_dir}")
            
            # Summary of key findings
            if hasattr(self, 'rs1_to_rs2_prediction') and self.rs1_to_rs2_prediction:
                r2_rs2 = self.rs1_to_rs2_prediction['avg_r2']
                print(f"ðŸŽ¯ Early biomarker performance (rs-1 â†’ rs-2): RÂ² = {r2_rs2:.3f}")
                
                if r2_rs2 > 0.3:
                    print("âœ… STRONG early biomarker potential detected!")
                elif r2_rs2 > 0.1:
                    print("âš ï¸ Moderate early biomarker potential detected")
                else:
                    print("âŒ Limited early biomarker potential with current approach")
            
            print("=" * 60)
            
            return {
                'success': True,
                'results_file': results_file,
                'report_file': report_file,
                'analysis_time': total_time,
                'early_biomarker_performance': early_biomarker_results
            }
            
        except Exception as e:
            print(f"\nâŒ ANALYSIS FAILED: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

def main():
    """Main entry point for longitudinal ICA compensation analysis"""
    
    # Initialize analyzer
    analyzer = LongitudinalICACompensationAnalyzer(
        data_dir="extracted data/raw_data",
        output_dir="longitudinal_ica_compensation_results"
    )
    
    # Run complete analysis
    results = analyzer.run_complete_analysis()
    
    if results['success']:
        print(f"\nðŸ“ All results saved to: longitudinal_ica_compensation_results/")
        print(f"ðŸ“„ Main report: {results['report_file']}")
        print(f"ðŸ“Š Detailed results: {results['results_file']}")
        
        print("\nðŸ”¬ RESEARCH GOAL ACHIEVED:")
        print("âœ… Individual SCA patient compensation strategies mapped")
        print("âœ… Early biomarkers identified from rs-1 data")
        print("âœ… Personalized compensation maps created")
        print("âœ… Longitudinal prediction models developed")
        print("âœ… Clinical translation pathway established")
        
    else:
        print(f"\nâŒ Analysis failed with error: {results['error']}")
        print("Please check the error details above and data availability.")

if __name__ == "__main__":
    main() 